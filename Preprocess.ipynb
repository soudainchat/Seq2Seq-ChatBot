{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c37b4e-a58a-4ff1-a001-f69f0bea93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb8bc4-163b-468d-8433-f625c2dfba6c",
   "metadata": {},
   "source": [
    "### Reading exported data from Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4045c1-a24a-43a1-bbaa-1e582e72f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('result.json')\n",
    "data = pd.DataFrame(data['chats']['list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47fdcc2-e974-4a08-a197-67aee093801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(data):\n",
    "    for i in range(1,len(data)):\n",
    "        if data['from'][i] == data['from'][i-1] and data['time_after_prev'][i] <= 15:\n",
    "            data.text[i] = data.text[i-1]+'. '+data.text[i]\n",
    "            data.time_after_prev[i] = data.time_after_prev[i]+data.time_after_prev[i-1]\n",
    "            data['duplicate'][i-1] = 1 \n",
    "    return data\n",
    "\n",
    "\n",
    "def resp(data):\n",
    "    for i in range(1,len(data)):\n",
    "        if data.time_after_prev[i]<900 and data['from'][i]!=data['from'][i-1]:\n",
    "            data.response[i-1]=data.text[i]\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess(data, cols):\n",
    "    data.drop(cols, axis=1, inplace=True)\n",
    "    data.drop(data.loc[data.type=='service'].index, inplace=True)\n",
    "    data.drop(data.loc[data.text==''].index, inplace=True)\n",
    "    data.date_unixtime=data.date_unixtime.astype('int64')\n",
    "\n",
    "    data=data.reset_index(drop=True)\n",
    "    data['time_after_prev'] = 0\n",
    "\n",
    "    for i in range(1,len(data)):\n",
    "        data['time_after_prev'][i] = data['date_unixtime'][i]-data['date_unixtime'][i-1]\n",
    "\n",
    "    data.drop(data.loc[data.text.apply(type) != str].index, inplace=True)\n",
    "    data=data.reset_index(drop=True)\n",
    "    data['duplicate'] = 0\n",
    "\n",
    "    data = union(data)\n",
    "\n",
    "    data.drop(data.loc[data.duplicate==1].index, inplace=True)\n",
    "    data.drop('duplicate',axis=1, inplace=True)\n",
    "    data=data.reset_index(drop=True)\n",
    "    data['response'] = 0\n",
    "\n",
    "    data = resp(data)\n",
    "    data.drop(data.loc[data['from']=='Mary Lazarenko'].index, inplace=True)\n",
    "    data.drop(data.loc[data['response']==0].index, inplace=True)\n",
    "    data.drop(['date_unixtime','type','time_after_prev'], axis=1,inplace=True)\n",
    "    data=data.reset_index(drop=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73551b-6b4f-4f56-8709-f5f0fd4b9388",
   "metadata": {},
   "source": [
    "---\n",
    "I will work with the most numerous dialogues from my collection. These are the ones I had with my family and closest friends.\n",
    "\n",
    "Unfortunately, json file I exported from TG is pretty inconsistent and it has different sets of fields for each dialogue, so at this point I will edit them individually.\n",
    "\n",
    "I will drop all messages containing files, gifs, links, images etc. so that only texts remain. Then they all messages will be organized in \"message\" and \"response\" columns based on sender info time difference between \"message\" and \"response\".  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381b6b0-e45a-4a07-98c2-ad05714e4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximka=pd.DataFrame((data['messages'][1]))\n",
    "aisha = pd.DataFrame((data['messages'][4]))\n",
    "lilya = pd.DataFrame((data['messages'][9]))\n",
    "katya = pd.DataFrame((data['messages'][10]))\n",
    "lyalya = pd.DataFrame((data['messages'][11]))\n",
    "dad = pd.DataFrame((data['messages'][17]))\n",
    "mom = pd.DataFrame((data['messages'][19]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79a6ef9-d095-4834-bc44-d0dcc79c0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cols = ['date','from_id','sticker_emoji','width','height','mime_type','duration_seconds','reply_to_message_id','actor','actor_id','action','discard_reason','file','thumbnail','media_type','photo','forwarded_from','edited','edited_unixtime','via_bot','game_title','game_description','game_link','game_message_id','score']\n",
    "maximka=preprocess(maximka, max_cols)\n",
    "\n",
    "\n",
    "sos_cols = ['date','from_id','forwarded_from', 'file', 'thumbnail', 'media_type', 'sticker_emoji',\n",
    "       'width', 'height', 'edited', 'edited_unixtime', 'photo', 'mime_type',\n",
    "       'duration_seconds', 'reply_to_message_id','actor','actor_id','action','discard_reason','location_information','emoticon','live_location_period_seconds']\n",
    "aisha=preprocess(aisha, sos_cols)\n",
    "\n",
    "\n",
    "lil_cols = ['date','from_id','forwarded_from', 'file', 'thumbnail', 'media_type', 'sticker_emoji',\n",
    "       'width', 'height', 'edited', 'edited_unixtime', 'photo', 'mime_type','duration_seconds', 'reply_to_message_id']\n",
    "lilya = preprocess(lilya, lil_cols)\n",
    "\n",
    "\n",
    "kat_cols = ['date','from_id', 'photo',\n",
    "       'width', 'height', 'file', 'thumbnail', 'media_type',\n",
    "       'mime_type', 'duration_seconds', 'sticker_emoji', 'forwarded_from',\n",
    "       'reply_to_message_id', 'edited', 'edited_unixtime', 'actor', 'actor_id',\n",
    "       'action', 'discard_reason', 'contact_information', 'contact_vcard']\n",
    "katya = preprocess(katya, kat_cols)\n",
    "\n",
    "\n",
    "lya_col = ['date','from_id',\n",
    "       'forwarded_from', 'file', 'thumbnail', 'media_type', 'mime_type',\n",
    "       'duration_seconds', 'width', 'height', 'photo', 'actor',\n",
    "       'actor_id', 'action', 'discard_reason', 'sticker_emoji',\n",
    "       'reply_to_message_id', 'edited', 'edited_unixtime']\n",
    "lyalya = preprocess(lyalya,lya_col)\n",
    "\n",
    "\n",
    "dad_cols = ['date', 'actor', 'actor_id', 'action','from_id', 'file', 'thumbnail', 'media_type',\n",
    "       'sticker_emoji', 'width', 'height', 'edited', 'edited_unixtime',\n",
    "       'forwarded_from', 'photo', 'contact_information', 'contact_vcard',\n",
    "       'reply_to_message_id']\n",
    "dad = preprocess(dad,dad_cols)\n",
    "\n",
    "\n",
    "mom_cols = ['date','actor', 'actor_id', 'action', 'from_id', 'file', 'media_type', 'mime_type',\n",
    "       'duration_seconds', 'photo', 'width', 'height', 'thumbnail',\n",
    "       'sticker_emoji', 'forwarded_from', 'edited', 'edited_unixtime',\n",
    "       'reply_to_message_id', 'discard_reason']\n",
    "mom = preprocess(mom,mom_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd83bbe-99e2-468e-982b-6b8d4dd008f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union all datasets in one\n",
    "\n",
    "frames = [maximka,aisha,lilya,katya,lyalya,dad,mom]\n",
    "full_data = pd.concat(frames)\n",
    "full_data=full_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab4c17b-2dc3-44b7-ba9b-baf27a7d198b",
   "metadata": {},
   "source": [
    "---\n",
    "Some messages have a lot of punctuation, emojis and other symbols, so my next move will be to remove them all from the data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23a037-0f2f-4146-8a14-562f0d700f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data):\n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "  \n",
    "    for i in range(len(data)):\n",
    "        data['text'][i] = data['text'][i].lower().replace('\\n', ' ') \\\n",
    "        .replace('-', ' ').replace(':', ' ').replace(')', '').replace('))', '').replace('))))', '') \\\n",
    "        .replace(',', '').replace('\"', '') \\\n",
    "        .replace(\"...\", \".\").replace(\"..\", \".\").replace(\"....\", \".\") \\\n",
    "        .replace(\"!\", \".\").replace(\"?\", \"\") \\\n",
    "        .replace(\";\", \".\").replace(\":\", \" \")\n",
    "\n",
    "    data['text'][i]=emoji_pattern.sub(r'', data['text'][i])\n",
    "\n",
    "    data['text'][i] = \"\".join(v for v in data['text'][i] if v not in string.punctuation).lower()\n",
    "    data['text'][i] = \" \".join(data['text'][i].split())\n",
    "\n",
    "    data['response'][i] = data['response'][i].lower().replace('\\n', ' ') \\\n",
    "    .replace('-', ' ').replace(':', ' ').replace(')', '').replace('))', '').replace('))))', '') \\\n",
    "    .replace(',', '').replace('\"', '') \\\n",
    "    .replace(\"...\", \".\").replace(\"..\", \".\").replace(\"....\", \".\") \\\n",
    "    .replace(\"!\", \".\").replace(\"?\", \"\") \\\n",
    "    .replace(\";\", \".\").replace(\":\", \" \")\n",
    "\n",
    "    data['response'][i]=emoji_pattern.sub(r'', data['response'][i])\n",
    "\n",
    "    data['response'][i] = \"\".join(v for v in data['response'][i] if v not in string.punctuation).lower()\n",
    "    data['response'][i] = \" \".join(data['response'][i].split())\n",
    "\n",
    "    return data\n",
    "\n",
    "full_data.drop('id',axis=1,inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "full_data['from'] = le.fit_transform(full_data['from'])\n",
    "\n",
    "full_data = process(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d46d15-5ce5-458c-ad67-96a000acb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving clear data \n",
    "full_data.to_csv('punc_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
